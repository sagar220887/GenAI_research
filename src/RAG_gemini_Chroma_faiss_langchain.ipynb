{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install streamlit pypdf2 -q\n",
    "# ! pip3 install streamlit google-generativeai python-dotenv langchain PyPDF2 chromadb faiss-cpu langchain_google_genai langchain-community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Loads .env file\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\")) # Loads API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_docs):\n",
    "    text = \" \"\n",
    "    # Iterate through each PDF document path in the list\n",
    "    for pdf in pdf_docs:\n",
    "        # Create a PdfReader object for the current PDF document\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        # Iterate through each page in the PDF document\n",
    "        for page in pdf_reader.pages:\n",
    "            # Extract text from the current page and append it to the 'text' string\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Return the concatenated text from all PDF documents\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):     \n",
    "    # Create embeddings using a Google Generative AI model\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    # Create a vector store using FAISS from the provided text chunks and embeddings\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "\n",
    "    # Save the vector store locally with the name \"faiss_index\"\n",
    "    vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain():\n",
    "    # Define a prompt template for asking questions based on a given context\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use ten sentences minimum and show the answer in bullet points and keep the answer concise.\\n\n",
    "    Question: {question} \\n\n",
    "    Context: {context} \\n\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a ChatGoogleGenerativeAI model for conversational AI\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "\n",
    "    # Create a prompt template with input variables \"context\" and \"question\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Load a question-answering chain with the specified model and prompt\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(user_question):\n",
    "    \n",
    "    # Create embeddings for the user question using a Google Generative AI model\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    # Load a FAISS vector database from a local file\n",
    "    new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "\n",
    "    # Perform similarity search in the vector database based on the user question\n",
    "    similar_docs = new_db.similarity_search(user_question)\n",
    "    # for doc in similar_docs:\n",
    "    #     print(doc)\n",
    "\n",
    "    # Obtain a conversational question-answering chain\n",
    "    chain = get_conversational_chain()\n",
    "\n",
    "    # Use the conversational chain to get a response based on the user question and retrieved documents\n",
    "    response = chain(\n",
    "        {\"input_documents\": similar_docs, \"question\": user_question}, return_only_outputs=True\n",
    "    )\n",
    "\n",
    "    # Print the response to the console\n",
    "    print(\"response - \", response[\"output_text\"])\n",
    "\n",
    "    # Display the response in a Streamlit app (assuming 'st' is a Streamlit module)\n",
    "    # st.write(\"Reply: \", response[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_docs = [os.path.join('data', 'Gen AI.pdf')]\n",
    "raw_text = get_pdf_text(pdf_docs)\n",
    "text_chunks = get_chunks(raw_text)\n",
    "get_vector_store(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response -      - Simple explainers are short, easy-to-understand descriptions that break down complex concepts into digestible chunks.\n",
      "    - Generative AI can help by quickly generating clear and concise explanations, making complicated topics more accessible to a wider audience.\n"
     ]
    }
   ],
   "source": [
    "user_input('What are Simple explainers?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
