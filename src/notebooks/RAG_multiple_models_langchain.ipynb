{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Local Documents - txt/pdf/csv etc\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! pip install -U -q google.generativeai chromadb langchain langchain-google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = os.path.join('..', 'data')\n",
    "FILENAME = 'yolo.pdf'\n",
    "\n",
    "FAISS_VECTORDB_PATH = './vectordb/faissdb'\n",
    "CHROMA_VECTORDB_PATH = './vectordb/chromadb'\n",
    "\n",
    "MODEL = \"mistral\" # mistral/gemini\n",
    "VECTORDB = \"faiss\" # faiss/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google API Key\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] =GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "- Read the documents form the local \n",
    "- Create the embeddings\n",
    "- Save the embeddings in the vectorDB - chromadb\n",
    "- Create a retriever object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='You Only Look Once:\\nUniﬁed, Real-Time Object Detection\\nJoseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\\nUniversity of Washington∗, Allen Institute for AI†, Facebook AI Research¶\\nhttp://pjreddie.com/yolo/\\nAbstract\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classiﬁers to per-\\nform detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and\\nassociated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from\\nfull images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end\\ndirectly on detection performance.\\nOur uniﬁed architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames\\nper second. A smaller version of the network, Fast YOLO,\\nprocesses an astounding 155 frames per second while\\nstill achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO\\nmakes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very\\ngeneral representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-\\nalizing from natural images to other domains like artwork.\\n1. Introduction\\nHumans glance at an image and instantly know what ob-\\njects are in the image, where they are, and how they inter-\\nact. The human visual system is fast and accurate, allow-\\ning us to perform complex tasks like driving with little con-\\nscious thought. Fast, accurate algorithms for object detec-\\ntion would allow computers to drive cars without special-\\nized sensors, enable assistive devices to convey real-time\\nscene information to human users, and unlock the potential\\nfor general purpose, responsive robotic systems.\\nCurrent detection systems repurpose classiﬁers to per-\\nform detection. To detect an object, these systems take a\\nclassiﬁer for that object and evaluate it at various locations\\nand scales in a test image. Systems like deformable parts\\nmodels (DPM) use a sliding window approach where the\\nclassiﬁer is run at evenly spaced locations over the entire\\nimage [10].\\nMore recent approaches like R-CNN use region proposal\\n1. Resize image.\\n2. Run convolutional network.3. Non-max suppression.\\nDog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\\nwith YOLO is simple and straightforward. Our system (1) resizes\\nthe input image to 448×448, (2) runs a single convolutional net-\\nwork on the image, and (3) thresholds the resulting detections by\\nthe model’s conﬁdence.\\nmethods to ﬁrst generate potential bounding boxes in an im-\\nage and then run a classiﬁer on these proposed boxes. After\\nclassiﬁcation, post-processing is used to reﬁne the bound-\\ning boxes, eliminate duplicate detections, and rescore the\\nboxes based on other objects in the scene [13]. These com-\\nplex pipelines are slow and hard to optimize because each\\nindividual component must be trained separately.\\nWe reframe object detection as a single regression prob-\\nlem, straight from image pixels to bounding box coordi-\\nnates and class probabilities. Using our system, you only\\nlook once (YOLO) at an image to predict what objects are\\npresent and where they are.\\nYOLO is refreshingly simple: see Figure 1. A sin-\\ngle convolutional network simultaneously predicts multi-\\nple bounding boxes and class probabilities for those boxes.\\nYOLO trains on full images and directly optimizes detec-\\ntion performance. This uniﬁed model has several beneﬁts\\nover traditional methods of object detection.\\nFirst, YOLO is extremely fast. Since we frame detection\\nas a regression problem we don’t need a complex pipeline.\\nWe simply run our neural network on a new image at test\\ntime to predict detections. Our base network runs at 45\\nframes per second with no batch processing on a Titan X\\nGPU and a fast version runs at more than 150 fps. This\\nmeans we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/ .\\nSecond, YOLO reasons globally about the image when\\n1arXiv:1506.02640v5  [cs.CV]  9 May 2016', metadata={'source': '..\\\\data\\\\yolo.pdf', 'page': 0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the documents\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "\n",
    "document_path = os.path.join(DATA_PATH, FILENAME)\n",
    "\n",
    "\n",
    "# loader = UnstructuredFileLoader(os.path.join(document_path, 'yolo.pdf'))\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(document_path)\n",
    "docs = loader.load()\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## get page content\n",
    "document_text = docs[0].page_content\n",
    "\n",
    "recursive_char_text_splitter=RecursiveCharacterTextSplitter(\n",
    "                                                chunk_size=500,\n",
    "                                                chunk_overlap=50)\n",
    "documents=recursive_char_text_splitter.split_documents(docs)\n",
    "\n",
    "# Convert the text to LangChain's `Document` format\n",
    "# docs =  [Document(page_content=document_text, metadata={\"source\": \"local\"})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "if MODEL == \"gemini\":\n",
    "        gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", api_key=GOOGLE_API_KEY)\n",
    "else:\n",
    "        embeddings=HuggingFaceEmbeddings(\n",
    "                model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "                model_kwargs={'device':'cpu'}\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gemini_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Save to disk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[0;32m      7\u001b[0m                      documents\u001b[38;5;241m=\u001b[39mdocuments,                 \u001b[38;5;66;03m# Data\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m                      embedding\u001b[38;5;241m=\u001b[39m\u001b[43mgemini_embeddings\u001b[49m,    \u001b[38;5;66;03m# Embedding model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m                      persist_directory\u001b[38;5;241m=\u001b[39mCHROMA_VECTORDB_PATH \u001b[38;5;66;03m# Directory to save data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                      )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m### 4. Create a retriever\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m## Load the vectorDB data from disk\u001b[39;00m\n\u001b[0;32m     15\u001b[0m vectorstore_disk \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[0;32m     16\u001b[0m                         persist_directory\u001b[38;5;241m=\u001b[39mCHROMA_VECTORDB_PATH,       \u001b[38;5;66;03m# Directory of db\u001b[39;00m\n\u001b[0;32m     17\u001b[0m                         embedding_function\u001b[38;5;241m=\u001b[39mgemini_embeddings   \u001b[38;5;66;03m# Embedding model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m                    )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gemini_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "## 3. Save Embeddings in vectorDB - chromadb\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=documents,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=CHROMA_VECTORDB_PATH # Directory to save data\n",
    "                     )\n",
    "\n",
    "### 4. Create a retriever\n",
    "\n",
    "## Load the vectorDB data from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=CHROMA_VECTORDB_PATH,       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "retriever_chromadb = vectorstore_disk.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same between YOLO and Fast YOLO.\n",
      "YOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\n",
      "Feature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\n",
      "R-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\n",
      "SDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\n",
      "sion of Fast R-CNN. Other versions of Fast R-CNN provide only\n",
      "a small beneﬁt while YOLO provides a signiﬁcant performance\n",
      "boost.\n"
     ]
    }
   ],
   "source": [
    "# print(len(retriever.get_relevant_documents(\"nvdia\")))\n",
    "retriever_documents = retriever_chromadb.get_relevant_documents(\"what is yolo?\")\n",
    "for doc in retriever_documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception inside storing data in vector db =>  The de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and no that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.).\n",
      "new_knowledge_base -  <langchain_community.vectorstores.faiss.FAISS object at 0x0000016B8A67FE10>\n"
     ]
    }
   ],
   "source": [
    "## 3. Save Embeddings in vectorDB - FAISS\n",
    "\n",
    "vector_db_directory = FAISS_VECTORDB_PATH\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def store_data_in_vectordb(documents, embeddings):\n",
    "    try:\n",
    "        current_vectordb = load_vectordb(vector_db_directory, embeddings)\n",
    "        print('current_vectordb - ', current_vectordb)\n",
    "    except Exception as e:\n",
    "        print('Exception inside storing data in vector db => ', e)\n",
    "\n",
    "    new_knowledge_base =FAISS.from_documents(documents, embeddings)\n",
    "    print('new_knowledge_base - ', new_knowledge_base)\n",
    "\n",
    "    # Saving the new vector DB\n",
    "    new_knowledge_base.save_local(vector_db_directory)\n",
    "    return new_knowledge_base\n",
    "\n",
    "\n",
    "def load_vectordb(stored_directory, embeddings):\n",
    "    loaded_vector_db = FAISS.load_local(stored_directory, embeddings)\n",
    "    return loaded_vector_db\n",
    "\n",
    "vector_db=store_data_in_vectordb(documents, gemini_embeddings)\n",
    "\n",
    "retriever_faissdb = vector_db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                chunk_id             document  page  \\\n",
      "0   2f8f317b-e6e9-482b-909c-41aefb0892bf  ..\\..\\data\\yolo.pdf     1   \n",
      "1   c170bce6-16a5-4cdc-970d-a37f98aa9bba  ..\\..\\data\\yolo.pdf     1   \n",
      "2   0b6a928b-c232-4558-b36f-e7bb25acac31  ..\\..\\data\\yolo.pdf     1   \n",
      "3   f1d67a3e-c288-4e14-a1f0-ac025a64e86d  ..\\..\\data\\yolo.pdf     1   \n",
      "4   123ce14f-f86c-40fb-a4f8-413e55b3190c  ..\\..\\data\\yolo.pdf     1   \n",
      "..                                   ...                  ...   ...   \n",
      "91  49756c4e-8713-49e0-a180-0833d77ca7a5  ..\\..\\data\\yolo.pdf     9   \n",
      "92  b3d8130e-ade3-411a-97a8-6ca48f76289f  ..\\..\\data\\yolo.pdf     9   \n",
      "93  3c3ec2b2-7a76-416a-9d8d-1a8de1a6cc7c  ..\\..\\data\\yolo.pdf    10   \n",
      "94  2bfc71a7-fcb0-404d-8bd2-cae05d4c6197  ..\\..\\data\\yolo.pdf    10   \n",
      "95  63caa967-b23b-42a5-8442-fab786ec1f3b  ..\\..\\data\\yolo.pdf    10   \n",
      "\n",
      "                                              content  \n",
      "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
      "1   associated class probabilities. A single neura...  \n",
      "2   still achieving double the mAP of other real-t...  \n",
      "3   jects are in the image, where they are, and ho...  \n",
      "4   for general purpose, responsive robotic system...  \n",
      "..                                                ...  \n",
      "91  abs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsk...  \n",
      "92  and Y . LeCun. Overfeat: Integrated recognitio...  \n",
      "93  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
      "94  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
      "95  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
      "\n",
      "[96 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2f8f317b-e6e9-482b-909c-41aefb0892bf</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>You Only Look Once:\\nUniﬁed, Real-Time Object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c170bce6-16a5-4cdc-970d-a37f98aa9bba</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>associated class probabilities. A single neura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0b6a928b-c232-4558-b36f-e7bb25acac31</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>still achieving double the mAP of other real-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1d67a3e-c288-4e14-a1f0-ac025a64e86d</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>jects are in the image, where they are, and ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123ce14f-f86c-40fb-a4f8-413e55b3190c</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>for general purpose, responsive robotic system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>49756c4e-8713-49e0-a180-0833d77ca7a5</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>abs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>b3d8130e-ade3-411a-97a8-6ca48f76289f</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>and Y . LeCun. Overfeat: Integrated recognitio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3c3ec2b2-7a76-416a-9d8d-1a8de1a6cc7c</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>[33] Z. Shen and X. Xue. Do more dropouts in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2bfc71a7-fcb0-404d-8bd2-cae05d4c6197</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>4\\n[36] P. Viola and M. Jones. Robust real-tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>63caa967-b23b-42a5-8442-fab786ec1f3b</td>\n",
       "      <td>..\\..\\data\\yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                chunk_id             document  page  \\\n",
       "0   2f8f317b-e6e9-482b-909c-41aefb0892bf  ..\\..\\data\\yolo.pdf     1   \n",
       "1   c170bce6-16a5-4cdc-970d-a37f98aa9bba  ..\\..\\data\\yolo.pdf     1   \n",
       "2   0b6a928b-c232-4558-b36f-e7bb25acac31  ..\\..\\data\\yolo.pdf     1   \n",
       "3   f1d67a3e-c288-4e14-a1f0-ac025a64e86d  ..\\..\\data\\yolo.pdf     1   \n",
       "4   123ce14f-f86c-40fb-a4f8-413e55b3190c  ..\\..\\data\\yolo.pdf     1   \n",
       "..                                   ...                  ...   ...   \n",
       "91  49756c4e-8713-49e0-a180-0833d77ca7a5  ..\\..\\data\\yolo.pdf     9   \n",
       "92  b3d8130e-ade3-411a-97a8-6ca48f76289f  ..\\..\\data\\yolo.pdf     9   \n",
       "93  3c3ec2b2-7a76-416a-9d8d-1a8de1a6cc7c  ..\\..\\data\\yolo.pdf    10   \n",
       "94  2bfc71a7-fcb0-404d-8bd2-cae05d4c6197  ..\\..\\data\\yolo.pdf    10   \n",
       "95  63caa967-b23b-42a5-8442-fab786ec1f3b  ..\\..\\data\\yolo.pdf    10   \n",
       "\n",
       "                                              content  \n",
       "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
       "1   associated class probabilities. A single neura...  \n",
       "2   still achieving double the mAP of other real-t...  \n",
       "3   jects are in the image, where they are, and ho...  \n",
       "4   for general purpose, responsive robotic system...  \n",
       "..                                                ...  \n",
       "91  abs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsk...  \n",
       "92  and Y . LeCun. Overfeat: Integrated recognitio...  \n",
       "93  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
       "94  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
       "95  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def convert_vectordb_to_df(vectorDb):\n",
    "    try:\n",
    "        vector_dict = vectorDb.docstore._dict\n",
    "        data_rows = []\n",
    "\n",
    "        for k in vector_dict.keys():\n",
    "            doc_name = vector_dict[k].metadata['source'].split('/')[-1]\n",
    "            if 'page' not in vector_dict[k].metadata:\n",
    "                page_number = 1\n",
    "            else:\n",
    "                page_number = vector_dict[k].metadata['page'] + 1\n",
    "            content =  vector_dict[k].page_content\n",
    "            data_rows.append({\"chunk_id\": k, \"document\": doc_name, \"page\": page_number, \"content\":content})\n",
    "\n",
    "        vector_df = pd.DataFrame(data_rows)\n",
    "        # print(vector_df)\n",
    "        return vector_df\n",
    "    except Exception as e :\n",
    "        print('Error in convert_vectordb_to_df - ', e)\n",
    "        return None\n",
    "    \n",
    "\n",
    "convert_vectordb_to_df(vector_db) # for faissdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['same between YOLO and Fast YOLO.',\n",
       " 'YOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\\nFeature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\\nR-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\\nSDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7',\n",
       " 'sion of Fast R-CNN. Other versions of Fast R-CNN provide only\\na small beneﬁt while YOLO provides a signiﬁcant performance\\nboost.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similiar_docs(vector_db, query,k=3,score=False):\n",
    "  \n",
    "    vectordb_results = list()\n",
    "    if score:\n",
    "        similar_docs_with_score = vector_db.similarity_search_with_score(query,k=k)\n",
    "        for doc, score in similar_docs_with_score:\n",
    "            print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")\n",
    "            vectordb_results.append(doc.page_content)\n",
    "    else:\n",
    "        similar_docs = vector_db.similarity_search(query,k=k)\n",
    "        for doc in similar_docs:\n",
    "            vectordb_results.append(doc.page_content)\n",
    "\n",
    "    return vectordb_results\n",
    "\n",
    "\n",
    "get_similiar_docs(vector_db, 'what is YOLO?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "## GENERATOR\n",
    "\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-pro\",\n",
    "#     temperature=0.1\n",
    "# )\n",
    "\n",
    "## Huggingface Model\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"You are an assistant for question-answering tasks.\\nUse the following context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse five sentences maximum and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "## prompt templates:\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema.prompt_template import format_document\n",
    "\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "\n",
    "# llm_prompt_template = \"\"\"You are an assistant for question-answering tasks. The documentation is located at SOURCE_DOCUMENTS.\n",
    "# You are given the following extracted parts of a long document and a question. \n",
    "# You should only use source and data that are explicitly listed as a source in the context. \n",
    "\n",
    "# Do NOT use any external resource, hyperlink or reference to answer that is not listed.\n",
    "\n",
    "# If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "    \n",
    "# {context}\n",
    "    \n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choosing a retriever\n",
    "\n",
    "# retriever = retriever_chromadb\n",
    "retriever = retriever_faissdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not mention what YOLO is, so I cannot answer this question from the provided context.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Create stuff documents chain using LCEL.\n",
    "# This is called a chain because you are chaining together different elements\n",
    "# with the LLM. In the following example, to create the stuff chain, you will\n",
    "# combine the relevant context from the website data matching the question, the\n",
    "# LLM model, and the output parser together like a chain using LCEL.\n",
    "#\n",
    "# The chain implements the following pipeline:\n",
    "# 1. Extract the website data relevant to the question from the Chroma\n",
    "#    vector store and save it to the variable `context`.\n",
    "# 2. `RunnablePassthrough` option to provide `question` when invoking\n",
    "#    the chain.\n",
    "# 3. The `context` and `question` are then passed to the prompt where they\n",
    "#    are populated in the respective variables.\n",
    "# 4. This prompt is then passed to the LLM (`gemini-pro`).\n",
    "# 5. Output from the LLM is passed through an output parser\n",
    "#    to structure the model's response.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "## inference\n",
    "rag_chain.invoke(\"What is YOLO?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'The provided context does not contain any information about what AI is, so I cannot answer this question from the provided context.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=retriever,\n",
    "            return_source_documents=False,\n",
    "            chain_type_kwargs={'prompt': llm_prompt}\n",
    "    )\n",
    "\n",
    "result=chain({'query':'What is ai?'}, return_only_outputs=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
