{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"yolo.pdf\")\n",
    "documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents length -- 16\n",
      "You Only Look Once:\n",
      "Uniﬁed, Real-Time Object Detection\n",
      "Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\n",
      "University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/\n",
      "Abstract\n",
      "We present YOLO, a new approach to object detection.\n",
      "Prior work on object detection repurposes classiﬁers to per-\n",
      "form detection. Instead, we frame object detection as a re-\n",
      "gression problem to spatially separated bounding boxes and\n",
      "associated class probabilities. A single neural network pre-\n",
      "dicts bounding boxes and class probabilities directly from\n",
      "full images in one evaluation. Since the whole detection\n",
      "pipeline is a single network, it can be optimized end-to-end\n",
      "directly on detection performance.\n",
      "Our uniﬁed architecture is extremely fast. Our base\n",
      "YOLO model processes images in real-time at 45 frames\n",
      "per second. A smaller version of the network, Fast YOLO,\n",
      "processes an astounding 155 frames per second while\n",
      "still achieving double the mAP of other real-time detec-\n",
      "tors. Compared to state-of-the-art detection systems, YOLO\n",
      "makes more localization errors but is less likely to predict\n",
      "false positives on background. Finally, YOLO learns very\n",
      "general representations of objects. It outperforms other de-\n",
      "tection methods, including DPM and R-CNN, when gener-\n",
      "alizing from natural images to other domains like artwork.\n",
      "1. Introduction\n",
      "Humans glance at an image and instantly know what ob-\n",
      "jects are in the image, where they are, and how they inter-\n",
      "act. The human visual system is fast and accurate, allow-\n",
      "ing us to perform complex tasks like driving with little con-\n",
      "scious thought. Fast, accurate algorithms for object detec-\n",
      "tion would allow computers to drive cars without special-\n",
      "ized sensors, enable assistive devices to convey real-time\n",
      "scene information to human users, and unlock the potential\n",
      "for general purpose, responsive robotic systems.\n",
      "Current detection systems repurpose classiﬁers to per-\n",
      "form detection. To detect an object, these systems take a\n",
      "classiﬁer for that object and evaluate it at various locations\n",
      "and scales in a test image. Systems like deformable parts\n",
      "models (DPM) use a sliding window approach where the\n",
      "classiﬁer is run at evenly spaced locations over the entire\n",
      "image [10].\n",
      "More recent approaches like R-CNN use region proposal\n",
      "1. Resize image.\n",
      "2. Run convolutional network.3. Non-max suppression.\n",
      "Dog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\n",
      "with YOLO is simple and straightforward. Our system (1) resizes\n",
      "the input image to 448×448, (2) runs a single convolutional net-\n",
      "work on the image, and (3) thresholds the resulting detections by\n",
      "the model’s conﬁdence.\n",
      "methods to ﬁrst generate potential bounding boxes in an im-\n",
      "age and then run a classiﬁer on these proposed boxes. After\n",
      "classiﬁcation, post-processing is used to reﬁne the bound-\n",
      "ing boxes, eliminate duplicate detections, and rescore the\n",
      "boxes based on other objects in the scene [13]. These com-\n",
      "plex pipelines are slow and hard to optimize because each\n",
      "individual component must be trained separately.\n",
      "We reframe object detection as a single regression prob-\n",
      "lem, straight from image pixels to bounding box coordi-\n",
      "nates and class probabilities. Using our system, you only\n",
      "look once (YOLO) at an image to predict what objects are\n",
      "present and where they are.\n",
      "YOLO is refreshingly simple: see Figure 1. A sin-\n",
      "gle convolutional network simultaneously predicts multi-\n",
      "ple bounding boxes and class probabilities for those boxes.\n",
      "YOLO trains on full images and directly optimizes detec-\n",
      "tion performance. This uniﬁed model has several beneﬁts\n",
      "over traditional methods of object detection.\n",
      "First, YOLO is extremely fast. Since we frame detection\n",
      "as a regression problem we don’t need a complex pipeline.\n",
      "We simply run our neural network on a new image at test\n",
      "time to predict detections. Our base network runs at 45\n",
      "frames per second with no batch processing on a Titan X\n"
     ]
    }
   ],
   "source": [
    "print(f'Documents length -- {len(documents)}')\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = RecursiveCharacterTextSplitter(chunk_size =512, chunk_overlap = 50)\n",
    "texts = split_text.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Only Look Once:\n",
      "Uniﬁed, Real-Time Object Detection\n",
      "Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\n",
      "University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/\n",
      "Abstract\n",
      "We present YOLO, a new approach to object detection.\n",
      "Prior work on object detection repurposes classiﬁers to per-\n",
      "form detection. Instead, we frame object detection as a re-\n",
      "gression problem to spatially separated bounding boxes and\n"
     ]
    }
   ],
   "source": [
    "print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embedding the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\Talk_with_your_data\\local_GPT_chatbot\\localgpt_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "    model_kwargs={'device':'cpu'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating vector db index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_documents(texts, embeddings)\n",
    "faiss_index_name = 'faiss_index_updated'\n",
    "\n",
    "faiss_index.save_local(faiss_index_name) # a directory shall be created in the name of <faiss_index_name> and couple of files are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loading the vector db from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vector_db_from_local = FAISS.load_local('./faiss_index_updated', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Viewing the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vectordb_to_df(vectorDb):\n",
    "    vector_dict = vectorDb.docstore._dict\n",
    "    data_rows = []\n",
    "\n",
    "    for k in vector_dict.keys():\n",
    "        doc_name = vector_dict[k].metadata['source'].split('/')[-1]\n",
    "        page_number = vector_dict[k].metadata['page'] + 1\n",
    "        content =  vector_dict[k].page_content\n",
    "        data_rows.append({\"chunk_id\": k, \"document\": doc_name, \"page\": page_number, \"content\":content})\n",
    "\n",
    "    vector_df = pd.DataFrame(data_rows)\n",
    "    print(vector_df)\n",
    "    return vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                chunk_id  document  page  \\\n",
      "0   75337173-28cd-44ca-b57f-6b38c307a6e1  yolo.pdf     1   \n",
      "1   ec546c79-8340-445f-ad01-b42d1db9e5b3  yolo.pdf     1   \n",
      "2   2ce49afc-d253-43fc-95e0-f80e6ba3d2a1  yolo.pdf     1   \n",
      "3   71bccb92-f842-44f7-b49c-efa8dc0ace50  yolo.pdf     1   \n",
      "4   d0d2901e-8048-46c3-be4a-84ffe589717b  yolo.pdf     1   \n",
      "..                                   ...       ...   ...   \n",
      "94  b3b09bf4-aba4-4fef-bbbb-d42389b344ce  yolo.pdf     9   \n",
      "95  5d01ef25-bbad-46d9-b0e2-917796c7d3d4  yolo.pdf     9   \n",
      "96  c0fe7989-80c7-4519-99f9-7f45325d1a7f  yolo.pdf    10   \n",
      "97  51341021-c471-48ae-b2b8-29dcb72486d9  yolo.pdf    10   \n",
      "98  cd987d5a-e5c6-4708-a1b4-9b9202156266  yolo.pdf    10   \n",
      "\n",
      "                                              content  \n",
      "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
      "1   associated class probabilities. A single neura...  \n",
      "2   still achieving double the mAP of other real-t...  \n",
      "3   jects are in the image, where they are, and ho...  \n",
      "4   for general purpose, responsive robotic system...  \n",
      "..                                                ...  \n",
      "94  5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun...  \n",
      "95  Recognition Challenge. International Journal o...  \n",
      "96  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
      "97  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
      "98  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
      "\n",
      "[99 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75337173-28cd-44ca-b57f-6b38c307a6e1</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>You Only Look Once:\\nUniﬁed, Real-Time Object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec546c79-8340-445f-ad01-b42d1db9e5b3</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>associated class probabilities. A single neura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ce49afc-d253-43fc-95e0-f80e6ba3d2a1</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>still achieving double the mAP of other real-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71bccb92-f842-44f7-b49c-efa8dc0ace50</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>jects are in the image, where they are, and ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d0d2901e-8048-46c3-be4a-84ffe589717b</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>for general purpose, responsive robotic system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>b3b09bf4-aba4-4fef-bbbb-d42389b344ce</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5d01ef25-bbad-46d9-b0e2-917796c7d3d4</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>Recognition Challenge. International Journal o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>c0fe7989-80c7-4519-99f9-7f45325d1a7f</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>[33] Z. Shen and X. Xue. Do more dropouts in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>51341021-c471-48ae-b2b8-29dcb72486d9</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>4\\n[36] P. Viola and M. Jones. Robust real-tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>cd987d5a-e5c6-4708-a1b4-9b9202156266</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                chunk_id  document  page  \\\n",
       "0   75337173-28cd-44ca-b57f-6b38c307a6e1  yolo.pdf     1   \n",
       "1   ec546c79-8340-445f-ad01-b42d1db9e5b3  yolo.pdf     1   \n",
       "2   2ce49afc-d253-43fc-95e0-f80e6ba3d2a1  yolo.pdf     1   \n",
       "3   71bccb92-f842-44f7-b49c-efa8dc0ace50  yolo.pdf     1   \n",
       "4   d0d2901e-8048-46c3-be4a-84ffe589717b  yolo.pdf     1   \n",
       "..                                   ...       ...   ...   \n",
       "94  b3b09bf4-aba4-4fef-bbbb-d42389b344ce  yolo.pdf     9   \n",
       "95  5d01ef25-bbad-46d9-b0e2-917796c7d3d4  yolo.pdf     9   \n",
       "96  c0fe7989-80c7-4519-99f9-7f45325d1a7f  yolo.pdf    10   \n",
       "97  51341021-c471-48ae-b2b8-29dcb72486d9  yolo.pdf    10   \n",
       "98  cd987d5a-e5c6-4708-a1b4-9b9202156266  yolo.pdf    10   \n",
       "\n",
       "                                              content  \n",
       "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
       "1   associated class probabilities. A single neura...  \n",
       "2   still achieving double the mAP of other real-t...  \n",
       "3   jects are in the image, where they are, and ho...  \n",
       "4   for general purpose, responsive robotic system...  \n",
       "..                                                ...  \n",
       "94  5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun...  \n",
       "95  Recognition Challenge. International Journal o...  \n",
       "96  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
       "97  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
       "98  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
       "\n",
       "[99 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_vectordb_to_df(loaded_vector_db_from_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Adding a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading new file - selenium_documentation_0.pdf\n",
    "loader = PyPDFLoader(\"selenium_documentation_0.pdf\")\n",
    "documents_new_doc = loader.load_and_split()\n",
    "\n",
    "# Splitting the documents\n",
    "split_text = RecursiveCharacterTextSplitter(chunk_size =512, chunk_overlap = 50)\n",
    "texts_new_docs = split_text.split_documents(documents=documents_new_doc)\n",
    "\n",
    "# Embedding the documents : using the same embedding used above\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "#     model_kwargs={'device':'cpu'}\n",
    "#     )\n",
    "\n",
    "faiss_index_new = FAISS.from_documents(texts_new_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the original vector db stored in local\n",
    "loaded_vector_db_from_local = FAISS.load_local('./faiss_index_updated', embeddings)\n",
    "\n",
    "# Merging the new vector db with the old one\n",
    "loaded_vector_db_from_local.merge_from(faiss_index_new)\n",
    "\n",
    "# Saving the merged vector db\n",
    "loaded_vector_db_from_local.save_local('./faiss_index_updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 chunk_id                      document  page  \\\n",
      "0    75337173-28cd-44ca-b57f-6b38c307a6e1                      yolo.pdf     1   \n",
      "1    ec546c79-8340-445f-ad01-b42d1db9e5b3                      yolo.pdf     1   \n",
      "2    2ce49afc-d253-43fc-95e0-f80e6ba3d2a1                      yolo.pdf     1   \n",
      "3    71bccb92-f842-44f7-b49c-efa8dc0ace50                      yolo.pdf     1   \n",
      "4    d0d2901e-8048-46c3-be4a-84ffe589717b                      yolo.pdf     1   \n",
      "..                                    ...                           ...   ...   \n",
      "844  b3cf81c2-4d76-4ea0-ae05-9efb6b0d7665  selenium_documentation_0.pdf   200   \n",
      "845  ace550c0-1f11-443c-be9c-f802e7317829  selenium_documentation_0.pdf   200   \n",
      "846  f6045114-dbad-404b-8b55-8cebcd8bbab2  selenium_documentation_0.pdf   200   \n",
      "847  df3a6312-dc71-46ba-b728-c5e2c1979b56  selenium_documentation_0.pdf   200   \n",
      "848  f03ed8b3-7dc5-44ab-8b65-22c6f2a8ee79  selenium_documentation_0.pdf   201   \n",
      "\n",
      "                                               content  \n",
      "0    You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
      "1    associated class probabilities. A single neura...  \n",
      "2    still achieving double the mAP of other real-t...  \n",
      "3    jects are in the image, where they are, and ho...  \n",
      "4    for general purpose, responsive robotic system...  \n",
      "..                                                 ...  \n",
      "844  WebDriverBackedSelenium and use a Sizzle locat...  \n",
      "845  is no longer possible. How can you tell if you...  \n",
      "846  or “document” directly.\\nAlternatively, you mi...  \n",
      "847  \"return arguments[0].tagName\" , element);\\nNot...  \n",
      "848  Selenium Documentation, Release 1.0\\nString ti...  \n",
      "\n",
      "[849 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75337173-28cd-44ca-b57f-6b38c307a6e1</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>You Only Look Once:\\nUniﬁed, Real-Time Object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec546c79-8340-445f-ad01-b42d1db9e5b3</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>associated class probabilities. A single neura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ce49afc-d253-43fc-95e0-f80e6ba3d2a1</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>still achieving double the mAP of other real-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71bccb92-f842-44f7-b49c-efa8dc0ace50</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>jects are in the image, where they are, and ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d0d2901e-8048-46c3-be4a-84ffe589717b</td>\n",
       "      <td>yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>for general purpose, responsive robotic system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>b3cf81c2-4d76-4ea0-ae05-9efb6b0d7665</td>\n",
       "      <td>selenium_documentation_0.pdf</td>\n",
       "      <td>200</td>\n",
       "      <td>WebDriverBackedSelenium and use a Sizzle locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>ace550c0-1f11-443c-be9c-f802e7317829</td>\n",
       "      <td>selenium_documentation_0.pdf</td>\n",
       "      <td>200</td>\n",
       "      <td>is no longer possible. How can you tell if you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>f6045114-dbad-404b-8b55-8cebcd8bbab2</td>\n",
       "      <td>selenium_documentation_0.pdf</td>\n",
       "      <td>200</td>\n",
       "      <td>or “document” directly.\\nAlternatively, you mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>df3a6312-dc71-46ba-b728-c5e2c1979b56</td>\n",
       "      <td>selenium_documentation_0.pdf</td>\n",
       "      <td>200</td>\n",
       "      <td>\"return arguments[0].tagName\" , element);\\nNot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>f03ed8b3-7dc5-44ab-8b65-22c6f2a8ee79</td>\n",
       "      <td>selenium_documentation_0.pdf</td>\n",
       "      <td>201</td>\n",
       "      <td>Selenium Documentation, Release 1.0\\nString ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 chunk_id                      document  page  \\\n",
       "0    75337173-28cd-44ca-b57f-6b38c307a6e1                      yolo.pdf     1   \n",
       "1    ec546c79-8340-445f-ad01-b42d1db9e5b3                      yolo.pdf     1   \n",
       "2    2ce49afc-d253-43fc-95e0-f80e6ba3d2a1                      yolo.pdf     1   \n",
       "3    71bccb92-f842-44f7-b49c-efa8dc0ace50                      yolo.pdf     1   \n",
       "4    d0d2901e-8048-46c3-be4a-84ffe589717b                      yolo.pdf     1   \n",
       "..                                    ...                           ...   ...   \n",
       "844  b3cf81c2-4d76-4ea0-ae05-9efb6b0d7665  selenium_documentation_0.pdf   200   \n",
       "845  ace550c0-1f11-443c-be9c-f802e7317829  selenium_documentation_0.pdf   200   \n",
       "846  f6045114-dbad-404b-8b55-8cebcd8bbab2  selenium_documentation_0.pdf   200   \n",
       "847  df3a6312-dc71-46ba-b728-c5e2c1979b56  selenium_documentation_0.pdf   200   \n",
       "848  f03ed8b3-7dc5-44ab-8b65-22c6f2a8ee79  selenium_documentation_0.pdf   201   \n",
       "\n",
       "                                               content  \n",
       "0    You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
       "1    associated class probabilities. A single neura...  \n",
       "2    still achieving double the mAP of other real-t...  \n",
       "3    jects are in the image, where they are, and ho...  \n",
       "4    for general purpose, responsive robotic system...  \n",
       "..                                                 ...  \n",
       "844  WebDriverBackedSelenium and use a Sizzle locat...  \n",
       "845  is no longer possible. How can you tell if you...  \n",
       "846  or “document” directly.\\nAlternatively, you mi...  \n",
       "847  \"return arguments[0].tagName\" , element);\\nNot...  \n",
       "848  Selenium Documentation, Release 1.0\\nString ti...  \n",
       "\n",
       "[849 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_vectordb_to_df(loaded_vector_db_from_local) ## both documents are present in this Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
