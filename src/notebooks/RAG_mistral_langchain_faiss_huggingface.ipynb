{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents length -  10\n",
      "page content -  You Only Look Once:\n",
      "Uniﬁed, Real-Time Object Detection\n",
      "Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\n",
      "University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/\n",
      "Abstract\n",
      "We present YOLO, a new approach to object detection.\n",
      "Prior work on object detection repurposes classiﬁers to per-\n",
      "form detection. Instead, we frame object detection as a re-\n",
      "gression problem to spatially separated bounding boxes and\n",
      "associated class probabilities. A single neural network pre-\n",
      "dicts bounding boxes and class probabilities directly from\n",
      "full images in one evaluation. Since the whole detection\n",
      "pipeline is a single network, it can be optimized end-to-end\n",
      "directly on detection performance.\n",
      "Our uniﬁed architecture is extremely fast. Our base\n",
      "YOLO model processes images in real-time at 45 frames\n",
      "per second. A smaller version of the network, Fast YOLO,\n",
      "processes an astounding 155 frames per second while\n",
      "still achieving double the mAP of other real-time detec-\n",
      "tors. Compared to state-of-the-art detection systems, YOLO\n",
      "makes more localization errors but is less likely to predict\n",
      "false positives on background. Finally, YOLO learns very\n",
      "general representations of objects. It outperforms other de-\n",
      "tection methods, including DPM and R-CNN, when gener-\n",
      "alizing from natural images to other domains like artwork.\n",
      "1. Introduction\n",
      "Humans glance at an image and instantly know what ob-\n",
      "jects are in the image, where they are, and how they inter-\n",
      "act. The human visual system is fast and accurate, allow-\n",
      "ing us to perform complex tasks like driving with little con-\n",
      "scious thought. Fast, accurate algorithms for object detec-\n",
      "tion would allow computers to drive cars without special-\n",
      "ized sensors, enable assistive devices to convey real-time\n",
      "scene information to human users, and unlock the potential\n",
      "for general purpose, responsive robotic systems.\n",
      "Current detection systems repurpose classiﬁers to per-\n",
      "form detection. To detect an object, these systems take a\n",
      "classiﬁer for that object and evaluate it at various locations\n",
      "and scales in a test image. Systems like deformable parts\n",
      "models (DPM) use a sliding window approach where the\n",
      "classiﬁer is run at evenly spaced locations over the entire\n",
      "image [10].\n",
      "More recent approaches like R-CNN use region proposal\n",
      "1. Resize image.\n",
      "2. Run convolutional network.3. Non-max suppression.\n",
      "Dog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\n",
      "with YOLO is simple and straightforward. Our system (1) resizes\n",
      "the input image to 448×448, (2) runs a single convolutional net-\n",
      "work on the image, and (3) thresholds the resulting detections by\n",
      "the model’s conﬁdence.\n",
      "methods to ﬁrst generate potential bounding boxes in an im-\n",
      "age and then run a classiﬁer on these proposed boxes. After\n",
      "classiﬁcation, post-processing is used to reﬁne the bound-\n",
      "ing boxes, eliminate duplicate detections, and rescore the\n",
      "boxes based on other objects in the scene [13]. These com-\n",
      "plex pipelines are slow and hard to optimize because each\n",
      "individual component must be trained separately.\n",
      "We reframe object detection as a single regression prob-\n",
      "lem, straight from image pixels to bounding box coordi-\n",
      "nates and class probabilities. Using our system, you only\n",
      "look once (YOLO) at an image to predict what objects are\n",
      "present and where they are.\n",
      "YOLO is refreshingly simple: see Figure 1. A sin-\n",
      "gle convolutional network simultaneously predicts multi-\n",
      "ple bounding boxes and class probabilities for those boxes.\n",
      "YOLO trains on full images and directly optimizes detec-\n",
      "tion performance. This uniﬁed model has several beneﬁts\n",
      "over traditional methods of object detection.\n",
      "First, YOLO is extremely fast. Since we frame detection\n",
      "as a regression problem we don’t need a complex pipeline.\n",
      "We simply run our neural network on a new image at test\n",
      "time to predict detections. Our base network runs at 45\n",
      "frames per second with no batch processing on a Titan X\n",
      "GPU and a fast version runs at more than 150 fps. This\n",
      "means we can process streaming video in real-time with\n",
      "less than 25 milliseconds of latency. Furthermore, YOLO\n",
      "achieves more than twice the mean average precision of\n",
      "other real-time systems. For a demo of our system running\n",
      "in real-time on a webcam please see our project webpage:\n",
      "http://pjreddie.com/yolo/ .\n",
      "Second, YOLO reasons globally about the image when\n",
      "1arXiv:1506.02640v5  [cs.CV]  9 May 2016\n"
     ]
    }
   ],
   "source": [
    "## 1. Data loading\n",
    "\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DATA_PATH = os.path.join('..', 'data')\n",
    "FILENAME = 'yolo.pdf'\n",
    "document_path = os.path.join(DATA_PATH, FILENAME)\n",
    "\n",
    "loader = PyPDFLoader(document_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print('Documents length - ', len(docs))\n",
    "document_text = docs[0].page_content\n",
    "print('page content - ', document_text)\n",
    "\n",
    "\n",
    "## 2. Chunking\n",
    "recursive_char_text_splitter=RecursiveCharacterTextSplitter(\n",
    "                                                chunk_size=500,\n",
    "                                                chunk_overlap=50)\n",
    "documents=recursive_char_text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 3. Embeddings - huggingface Embeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "                model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "                model_kwargs={'device':'cpu'}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_knowledge_base -  <langchain_community.vectorstores.faiss.FAISS object at 0x000001CBEEAEA9D0>\n"
     ]
    }
   ],
   "source": [
    "## 3. Save Embeddings in vectorDB - FAISS\n",
    "\n",
    "FAISS_VECTORDB_PATH = os.path.join('vectordb', 'faiss_vector_db')\n",
    "\n",
    "vector_db_directory = FAISS_VECTORDB_PATH\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def store_data_in_vectordb(documents, embeddings):\n",
    "    # try:\n",
    "    #     current_vectordb = load_vectordb(vector_db_directory, embeddings)\n",
    "    #     print('current_vectordb - ', current_vectordb)\n",
    "    # except Exception as e:\n",
    "    #     print('Exception inside storing data in vector db => ', e)\n",
    "\n",
    "    new_knowledge_base =FAISS.from_documents(documents, embeddings)\n",
    "    print('new_knowledge_base - ', new_knowledge_base)\n",
    "\n",
    "    # Saving the new vector DB\n",
    "    new_knowledge_base.save_local(vector_db_directory)\n",
    "    return new_knowledge_base\n",
    "\n",
    "\n",
    "def load_vectordb(stored_directory, embeddings):\n",
    "    loaded_vector_db = FAISS.load_local(stored_directory, embeddings, allow_dangerous_deserialization=True)\n",
    "    return loaded_vector_db\n",
    "\n",
    "store_data_in_vectordb(documents, embeddings) # storing data into vectorDB\n",
    "vector_db = load_vectordb(vector_db_directory, embeddings) # reading data from vectordb database\n",
    "\n",
    "retriever_faissdb = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d5a5a571-cd30-4b88-8651-85030c7e193a</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>You Only Look Once:\\nUniﬁed, Real-Time Object ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>787504a2-d594-4ba9-875c-45f99682b85b</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>associated class probabilities. A single neura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9f64f170-2569-4e75-b928-529c40e19b11</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>still achieving double the mAP of other real-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d5636b5f-f6c8-4b54-bac3-ac4fbd8fc784</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>jects are in the image, where they are, and ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4da755aa-ab4e-43bd-9130-cd51fee17583</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>for general purpose, responsive robotic system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>d4c8fa44-4413-4361-a2ba-479bc64c5902</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>abs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2cd73001-8612-4274-a4ef-55ce336bd3d3</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>and Y . LeCun. Overfeat: Integrated recognitio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>00e57314-d73d-4c7d-aba7-d61cba180fac</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>[33] Z. Shen and X. Xue. Do more dropouts in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>97798490-d18d-4502-ba79-565d5bad9142</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>4\\n[36] P. Viola and M. Jones. Robust real-tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>c6a86747-ee29-4bea-b984-820311d48baa</td>\n",
       "      <td>..\\data\\yolo.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                chunk_id          document  page  \\\n",
       "0   d5a5a571-cd30-4b88-8651-85030c7e193a  ..\\data\\yolo.pdf     1   \n",
       "1   787504a2-d594-4ba9-875c-45f99682b85b  ..\\data\\yolo.pdf     1   \n",
       "2   9f64f170-2569-4e75-b928-529c40e19b11  ..\\data\\yolo.pdf     1   \n",
       "3   d5636b5f-f6c8-4b54-bac3-ac4fbd8fc784  ..\\data\\yolo.pdf     1   \n",
       "4   4da755aa-ab4e-43bd-9130-cd51fee17583  ..\\data\\yolo.pdf     1   \n",
       "..                                   ...               ...   ...   \n",
       "91  d4c8fa44-4413-4361-a2ba-479bc64c5902  ..\\data\\yolo.pdf     9   \n",
       "92  2cd73001-8612-4274-a4ef-55ce336bd3d3  ..\\data\\yolo.pdf     9   \n",
       "93  00e57314-d73d-4c7d-aba7-d61cba180fac  ..\\data\\yolo.pdf    10   \n",
       "94  97798490-d18d-4502-ba79-565d5bad9142  ..\\data\\yolo.pdf    10   \n",
       "95  c6a86747-ee29-4bea-b984-820311d48baa  ..\\data\\yolo.pdf    10   \n",
       "\n",
       "                                              content  \n",
       "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
       "1   associated class probabilities. A single neura...  \n",
       "2   still achieving double the mAP of other real-t...  \n",
       "3   jects are in the image, where they are, and ho...  \n",
       "4   for general purpose, responsive robotic system...  \n",
       "..                                                ...  \n",
       "91  abs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsk...  \n",
       "92  and Y . LeCun. Overfeat: Integrated recognitio...  \n",
       "93  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
       "94  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
       "95  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data in vector DB\n",
    "\n",
    "import pandas as pd\n",
    "def convert_vectordb_to_df(vectorDb):\n",
    "    try:\n",
    "        vector_dict = vectorDb.docstore._dict\n",
    "        data_rows = []\n",
    "\n",
    "        for k in vector_dict.keys():\n",
    "            doc_name = vector_dict[k].metadata['source'].split('/')[-1]\n",
    "            if 'page' not in vector_dict[k].metadata:\n",
    "                page_number = 1\n",
    "            else:\n",
    "                page_number = vector_dict[k].metadata['page'] + 1\n",
    "            content =  vector_dict[k].page_content\n",
    "            data_rows.append({\"chunk_id\": k, \"document\": doc_name, \"page\": page_number, \"content\":content})\n",
    "\n",
    "        vector_df = pd.DataFrame(data_rows)\n",
    "        return vector_df\n",
    "    except Exception as e :\n",
    "        print('Error in convert_vectordb_to_df - ', e)\n",
    "        return None\n",
    "    \n",
    "\n",
    "convert_vectordb_to_df(vector_db) # for faissdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['same between YOLO and Fast YOLO.',\n",
       " 'Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\\ndoes think one person is an airplane.\\nincluding the time to fetch images from the camera and dis-\\nplay the detections.\\nThe resulting system is interactive and engaging. While\\nYOLO processes images individually, when attached to a\\nwebcam it functions like a tracking system, detecting ob-\\njects as they move around and change in appearance. A',\n",
       " 'making predictions. Unlike sliding window and region\\nproposal-based techniques, YOLO sees the entire image\\nduring training and test time so it implicitly encodes contex-\\ntual information about classes as well as their appearance.\\nFast R-CNN, a top detection method [14], mistakes back-\\nground patches in an image for objects because it can’t see\\nthe larger context. YOLO makes less than half the number\\nof background errors compared to Fast R-CNN.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similiar_docs(vector_db, query,k=3,score=False):\n",
    "  \n",
    "    vectordb_results = list()\n",
    "    if score:\n",
    "        similar_docs_with_score = vector_db.similarity_search_with_score(query,k=k)\n",
    "        for doc, score in similar_docs_with_score:\n",
    "            print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")\n",
    "            vectordb_results.append(doc.page_content)\n",
    "    else:\n",
    "        similar_docs = vector_db.similarity_search(query,k=k)\n",
    "        for doc in similar_docs:\n",
    "            vectordb_results.append(doc.page_content)\n",
    "\n",
    "    return vectordb_results\n",
    "\n",
    "\n",
    "get_similiar_docs(vector_db, 'what is YOLO?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"You are an assistant for question-answering tasks.\\nUse the following context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences minimum and show the answer in bullet points and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## prompt templates:\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema.prompt_template import format_document\n",
    "\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use ten sentences minimum and show the answer in bullet points and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks.\n",
      "Use the following context to answer the question.\n",
      "If you don't know the answer, just say that you don't know.\n",
      "Use ten sentences minimum and show the answer in bullet points and keep the answer concise.\n",
      "\n",
      "Question: What is YOLO? \n",
      "Context: same between YOLO and Fast YOLO.\n",
      "\n",
      "Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\n",
      "does think one person is an airplane.\n",
      "including the time to fetch images from the camera and dis-\n",
      "play the detections.\n",
      "The resulting system is interactive and engaging. While\n",
      "YOLO processes images individually, when attached to a\n",
      "webcam it functions like a tracking system, detecting ob-\n",
      "jects as they move around and change in appearance. A \n",
      "Answer:\n",
      "- YOLO stands for You Only Look Once, which is a real-time object detection system.\n",
      "- It was developed by Joseph Redmon and Ali Farhadi in 2015.\n",
      "- YOLO uses a single neural network to predict bounding boxes and class probabilities directly from full images.\n",
      "- It is faster than other object detection systems like Faster R-CNN and R-FCN because it processes images in one pass.\n",
      "- YOLO can\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=retriever_faissdb,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={'prompt': llm_prompt}\n",
    "    )\n",
    "\n",
    "result=chain({'query':'What is YOLO?'}, return_only_outputs=True)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
