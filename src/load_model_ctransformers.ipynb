{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ctransformers\n",
      "  Using cached ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-5.26.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting huggingface-hub (from ctransformers)\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting py-cpuinfo<10.0.0,>=9.0.0 (from ctransformers)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\sagar\\work\\ai_ml\\gitrepo\\genai\\youtubes\\genai_research\\venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\sagar\\work\\ai_ml\\gitrepo\\genai\\youtubes\\genai_research\\venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.4.16-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.0 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 675.1 kB/s eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\sagar\\work\\ai_ml\\gitrepo\\genai\\youtubes\\genai_research\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
      "Using cached torch-2.2.2-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "Using cached protobuf-5.26.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Downloading regex-2024.4.16-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 194.6/268.9 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.9/268.9 kB 4.1 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: sentencepiece, py-cpuinfo, mpmath, urllib3, tqdm, sympy, safetensors, regex, pyyaml, protobuf, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, ctransformers, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.2.2 charset-normalizer-3.3.2 ctransformers-0.2.27 filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 idna-3.7 jinja2-3.1.3 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 protobuf-5.26.1 py-cpuinfo-9.0.0 pyyaml-6.0.1 regex-2024.4.16 requests-2.31.0 safetensors-0.4.3 sentencepiece-0.2.0 sympy-1.12 tokenizers-0.15.2 torch-2.2.2 tqdm-4.66.2 transformers-4.39.3 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install ctransformers torch transformers sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\", model_file=\"openhermes-2.5-mistral-7b.Q2_K.gguf\")\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\", hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(pipe(\"AI is going to\", max_new_tokens=256))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
