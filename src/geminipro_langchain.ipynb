{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q google-generativeai langchain-google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] =GOOGLE_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\GenAI_research\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name = \"gemini-pro\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Mixture of Experts (MoE)**\\n\\n**Definition:**\\n\\nMixture of Experts is a machine learning technique that utilizes a hierarchical model to combine the predictions of multiple sub-models (experts).\\n\\n**Architecture:**\\n\\n* **Gate Network:** A neural network that determines the contribution of each expert to the final prediction.\\n* **Expert Networks:** Multiple sub-networks that specialize in different parts of the data or task.\\n* **Mixing Layer:** Combines the predictions of the experts based on the gate network's output.\\n\\n**Working Principle:**\\n\\n1. **Training Gate Network:** The gate network is trained to predict the optimal mixing weights for each expert.\\n2. **Expert Predictions:** Each expert network makes predictions on the input data.\\n3. **Weighting and Mixing:** The gate network calculates the mixing weights and combines the expert predictions accordingly.\\n4. **Final Prediction:** The mixed prediction is considered the final output of the MoE model.\\n\\n**Advantages:**\\n\\n* **Improved Accuracy:** By combining the predictions of multiple experts, MoE enhances overall accuracy.\\n* **Model Complexity Reduction:** Avoids training a single large model by leveraging multiple smaller experts.\\n* **Data Specialization:** Experts can be tailored to specific tasks or data subsets.\\n* **Interpretability:** Gate network weights provide insights into expert contributions.\\n\\n**Applications:**\\n\\n* Image classification\\n* Natural language processing\\n* Recommendation systems\\n* Time series forecasting\\n* Reinforcement learning\\n\\n**Variants:**\\n\\n* Gaussian Mixture of Experts (GMoE)\\n* Deep Mixture of Experts (DMoE)\\n* Conditional Mixture of Experts (CMoE)\\n* Gated Mixture of Experts (GMoX)\\n\\n**Note:** MoE is closely related to Gating Networks and Ensemble Learning, where multiple models are combined to improve performance.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\n",
    "    \"What is Mixture of Experts?\",\n",
    "]\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is Mixture of Experts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Mixture of Experts (MoE)** is a machine learning model that combines multiple expert models to make predictions. It is a type of ensemble model, which means it uses multiple models to improve performance.\n",
      "\n",
      "**How Mixture of Experts Works:**\n",
      "\n",
      "1. **Expert Models:** The MoE consists of multiple expert models, each trained on a specific task or aspect of the data.\n",
      "2. **Gating Network:** A gating network is used to determine which expert model is most suitable for each input data point.\n",
      "3. **Weighted Sum:** The output of the MoE is a weighted sum of the predictions from the expert models. The weights are determined by the gating network.\n",
      "\n",
      "**Benefits of Mixture of Experts:**\n",
      "\n",
      "* **Improved Performance:** By combining multiple experts, MoEs can model complex relationships in the data and achieve higher accuracy.\n",
      "* **Interpretability:** MoEs provide insights into the different experts contributing to the prediction, making it easier to understand the model's behavior.\n",
      "* **Scalability:** MoEs can be trained on large datasets by distributing the training across the expert models.\n",
      "\n",
      "**Applications of Mixture of Experts:**\n",
      "\n",
      "* Image classification\n",
      "* Natural language processing\n",
      "* Speech recognition\n",
      "* Medical diagnosis\n",
      "* Recommendation systems\n",
      "\n",
      "**Advantages of Mixture of Experts:**\n",
      "\n",
      "* **Flexible:** MoEs can be customized by choosing different expert models and gating networks.\n",
      "* **Data-efficient:** By using multiple experts, MoEs can learn from smaller datasets.\n",
      "* **Robust:** MoEs are less susceptible to overfitting and noise than single models.\n",
      "\n",
      "**Disadvantages of Mixture of Experts:**\n",
      "\n",
      "* **Computational Cost:** Training and inference can be computationally expensive due to the multiple expert models.\n",
      "* **Complexity:** MoEs can be complex to design and tune.\n",
      "* **Interpretability Challenges:** Understanding the contributions of each expert model can be difficult.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain + Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' A snow-capped mountain at sunset.', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-701cee37-d7f2-415c-a8ba-22d0cfbea3c8-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "# example\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What's in this image?\",\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\", \n",
    "            \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: How to show an image from a given url within markdown\n",
    "\n",
    "# import requests\n",
    "# import io\n",
    "# from PIL import Image\n",
    "\n",
    "# def show_image(url):\n",
    "#   response = requests.get(url)\n",
    "#   image = Image.open(io.BytesIO(response.content))\n",
    "#   display(image)\n",
    "\n",
    "# show_image(\"https://tinyurl.com/xpb6vjdc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' A snow-capped mountain peak.', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-dcc8628d-b80f-402c-b966-023eca131342-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# picture description\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "# example\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What do you see in this image?\",\n",
    "        },\n",
    "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/150\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = llm.invoke([message])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A mountain peak covered in snow.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The mountain stood tall and proud, its peak reaching up to the heavens. It was a symbol of strength and resilience, and it had stood there for centuries. The mountain had seen many things in its time. It had seen the rise and fall of civilizations, the coming and going of wars, and the birth and death of countless people. But through it all, the mountain had remained unchanged. It was a constant in a world of change.\n",
      "\n",
      "One day, a young man came to the mountain. He was a climber, and he had come to conquer the mountain. The young man was determined to reach the summit, and he was not going to let anything stop him. He began his climb, and he quickly made his way up the mountain. But as he got higher and higher, the climb became more and more difficult. The air became thinner, and the wind became stronger. The young man was starting to get tired, but he kept going. He was determined to reach the summit.\n",
      "\n",
      "Finally, after hours of climbing, the young man reached the summit of the mountain. He was exhausted, but he was also exhilarated. He had achieved his goal, and he had proven to himself that he could do anything he set his mind to. The young man stood at the summit of the mountain and looked out at the world below. He felt a sense of peace and tranquility. He knew that he would never forget this moment.\n",
      "\n",
      "The young man's story is a reminder that anything is possible if you set your mind to it. No matter how difficult the challenge, if you keep going, you will eventually achieve your goal. So never give up on your dreams. Keep climbing, and you will eventually reach the summit.\n"
     ]
    }
   ],
   "source": [
    "## short story content\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "# example\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"What do you see in this image? Create a short story based on the content\",\n",
    "        },\n",
    "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/150\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = llm.invoke([message])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
